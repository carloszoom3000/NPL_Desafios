{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Procesamiento Natural del Lenguaje**\n",
        "\n",
        "**Desafio 4**\n",
        "\n",
        "**Carlos Villalobos**"
      ],
      "metadata": {
        "id": "DChNUw_V37dL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Consigna.**"
      ],
      "metadata": {
        "id": "Jxg2Kdxb4JEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**2 - Preprocesamiento**\n",
        "\n",
        "Realizar el preprocesamiento necesario para obtener:\n",
        "\n",
        "    word2idx_inputs, max_input_len\n",
        "    word2idx_outputs, max_out_len, num_words_output\n",
        "    encoder_input_sequences, decoder_output_sequences, decoder_targets\n",
        "\n"
      ],
      "metadata": {
        "id": "XHCLMCy14HHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**3 - Preparar los embeddings**\n",
        "\n",
        "Utilizar los embeddings de Glove o FastText para transformar los tokens de entrada en vectores\n"
      ],
      "metadata": {
        "id": "AJDzItxf4rPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**4 - Entrenar el modelo**\n",
        "\n",
        "Entrenar un modelo basado en el esquema encoder-decoder utilizando los datos generados en los puntos anteriores. Utilce como referencias los ejemplos vistos en clase.\n"
      ],
      "metadata": {
        "id": "9TRr4vhM411L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**5 - Inferencia**\n",
        "\n",
        "Experimentar el funcionamiento de su modelo. Recuerde que debe realizar la inferencia de los modelos por separado de encoder y decoder.\n"
      ],
      "metadata": {
        "id": "X3QHAn5f47oY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "# from keras.preprocessing.text import one_hot # esto causa error\n",
        "from tensorflow.keras.preprocessing.text import one_hot # uso entonces tensorflow.keras\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, LSTM, SimpleRNN\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from keras.preprocessing.text import Tokenizer # esto también causa error\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # uso entonces tensorflow.keras\n",
        "from keras.layers import Input"
      ],
      "metadata": {
        "id": "L_boRxOmyFMY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar la carpeta de dataset\n",
        "import os\n",
        "import gdown\n",
        "if os.access('data_volunteers.json', os.F_OK) is False:\n",
        "    url = 'https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download'\n",
        "    output = 'data_volunteers.json'\n",
        "    gdown.download(url, output, quiet=False)\n",
        "else:\n",
        "    print(\"El dataset ya se encuentra descargado\")\n",
        "\n",
        "\n",
        "# dataset_file\n",
        "import json\n",
        "\n",
        "text_file = \"data_volunteers.json\"\n",
        "with open(text_file) as f:\n",
        "    data = json.load(f) # la variable data será un diccionario"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UqkYJRDyTW-",
        "outputId": "d0ca8cf3-333b-40b5-c605-19d9d54b10bf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El dataset ya se encuentra descargado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Observar los campos disponibles en cada linea del dataset\n",
        "data[0].keys()\n",
        "\n",
        "\n",
        "chat_in = []\n",
        "chat_out = []\n",
        "\n",
        "input_sentences = []\n",
        "output_sentences = []\n",
        "output_sentences_inputs = []\n",
        "max_len = 30\n",
        "\n",
        "def clean_text(txt):\n",
        "    txt = txt.lower()\n",
        "    txt.replace(\"\\'d\", \" had\")\n",
        "    txt.replace(\"\\'s\", \" is\")\n",
        "    txt.replace(\"\\'m\", \" am\")\n",
        "    txt.replace(\"don't\", \"do not\")\n",
        "    txt = re.sub(r'\\W+', ' ', txt)\n",
        "\n",
        "    return txt\n",
        "\n",
        "for line in data:\n",
        "    for i in range(len(line['dialog'])-1):\n",
        "        # vamos separando el texto en \"preguntas\" (chat_in)\n",
        "        # y \"respuestas\" (chat_out)\n",
        "        chat_in = clean_text(line['dialog'][i]['text'])\n",
        "        chat_out = clean_text(line['dialog'][i+1]['text'])\n",
        "\n",
        "        if len(chat_in) >= max_len or len(chat_out) >= max_len:\n",
        "            continue\n",
        "\n",
        "        input_sentence, output = chat_in, chat_out\n",
        "\n",
        "        # output sentence (decoder_output) tiene\n",
        "        output_sentence = output + ' '\n",
        "        # output sentence input (decoder_input) tiene\n",
        "        output_sentence_input = ' ' + output\n",
        "\n",
        "        input_sentences.append(input_sentence)\n",
        "        output_sentences.append(output_sentence)\n",
        "        output_sentences_inputs.append(output_sentence_input)\n",
        "\n",
        "print(\"Cantidad de rows utilizadas:\", len(input_sentences))\n",
        "\n",
        "\n",
        "input_sentences[1], output_sentences[1], output_sentences_inputs[1]\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YxYDuSKyiZ8",
        "outputId": "04835b93-c88c-4e23-c7d3-421a6d79de0f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de rows utilizadas: 6033\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('hi how are you ', 'not bad and you  ', ' not bad and you ')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 - Preprocesamiento\n",
        "tokenizer_inputs = Tokenizer()\n",
        "tokenizer_inputs.fit_on_texts(input_sentences)\n",
        "input_sequences = tokenizer_inputs.texts_to_sequences(input_sentences)\n",
        "\n",
        "word2idx_inputs = tokenizer_inputs.word_index\n",
        "max_input_len = max(len(seq) for seq in input_sequences)\n",
        "encoder_input_sequences = pad_sequences(input_sequences, maxlen=max_input_len, padding='post')\n",
        "\n",
        "tokenizer_outputs = Tokenizer()\n",
        "tokenizer_outputs.fit_on_texts(output_sentences + output_sentences_inputs)\n",
        "output_sequences = tokenizer_outputs.texts_to_sequences(output_sentences)\n",
        "output_sequences_inputs = tokenizer_outputs.texts_to_sequences(output_sentences_inputs)\n",
        "\n",
        "word2idx_outputs = tokenizer_outputs.word_index\n",
        "num_words_output = len(word2idx_outputs) + 1\n",
        "max_out_len = max(len(seq) for seq in output_sequences)\n",
        "\n",
        "decoder_output_sequences = pad_sequences(output_sequences, maxlen=max_out_len, padding='post')\n",
        "decoder_targets = decoder_output_sequences.reshape(decoder_output_sequences.shape[0], decoder_output_sequences.shape[1], 1)"
      ],
      "metadata": {
        "id": "lFJTcOdZyn9B"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idscLADAzSfP",
        "outputId": "740b1fc6-b40f-45ad-93d8-f2c5d036d121"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 - Preparar los embeddings\n",
        "# Asumimos que estamos usando GloVe embeddings de 100 dimensiones\n",
        "import os\n",
        "import gdown\n",
        "\n",
        "# Check if the file already exists, if not, download it\n",
        "if not os.path.exists('glove.6B.100d.txt'):\n",
        "    url = 'https://nlp.stanford.edu/data/glove.6B.zip'\n",
        "    output = 'glove.6B.zip'\n",
        "    gdown.download(url, output, quiet=False)\n",
        "\n",
        "    # Unzip the downloaded file\n",
        "    !unzip glove.6B.zip\n",
        "\n",
        "embeddings_index = {}\n",
        "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((num_words_output, embedding_dim))\n",
        "for word, i in word2idx_outputs.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKreVOGezXVD",
        "outputId": "2b77abd6-7aa9-49fb-f677-2e56cd24e7a4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://nlp.stanford.edu/data/glove.6B.zip\n",
            "To: /content/glove.6B.zip\n",
            "100%|██████████| 862M/862M [02:38<00:00, 5.43MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 - Entrenar el modelo\n",
        "# Definición del modelo\n",
        "latent_dim = 256\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_input_len,))\n",
        "encoder_embedding = Embedding(num_words_output, embedding_dim, weights=[embedding_matrix], trainable=False)(encoder_inputs)\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(num_words_output, embedding_dim, weights=[embedding_matrix], trainable=False)(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_words_output, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "metadata": {
        "id": "DQCGXh1G0Spg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelo completo\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compilación y entrenamiento\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "model.fit([encoder_input_sequences, decoder_output_sequences[:, :-1]], decoder_targets[:, 1:],\n",
        "          batch_size=64, epochs=50, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-06rT1-0XyQ",
        "outputId": "9aa71c66-b05e-41ee-8a04-c4e666556ac6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - loss: 3.4431 - val_loss: 2.2415\n",
            "Epoch 2/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 1.9746 - val_loss: 1.9695\n",
            "Epoch 3/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1.7802 - val_loss: 1.8688\n",
            "Epoch 4/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1.6039 - val_loss: 1.7566\n",
            "Epoch 5/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1.4793 - val_loss: 1.7092\n",
            "Epoch 6/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1.4271 - val_loss: 1.6610\n",
            "Epoch 7/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1.3905 - val_loss: 1.6212\n",
            "Epoch 8/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1.3248 - val_loss: 1.6029\n",
            "Epoch 9/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1.3217 - val_loss: 1.5698\n",
            "Epoch 10/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 1.2759 - val_loss: 1.5805\n",
            "Epoch 11/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.2188 - val_loss: 1.5447\n",
            "Epoch 12/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.1843 - val_loss: 1.5125\n",
            "Epoch 13/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.1975 - val_loss: 1.4958\n",
            "Epoch 14/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.1405 - val_loss: 1.4809\n",
            "Epoch 15/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.1378 - val_loss: 1.4734\n",
            "Epoch 16/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.1345 - val_loss: 1.4691\n",
            "Epoch 17/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1.1276 - val_loss: 1.4604\n",
            "Epoch 18/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1.0904 - val_loss: 1.4473\n",
            "Epoch 19/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1.0354 - val_loss: 1.4403\n",
            "Epoch 20/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1.0561 - val_loss: 1.4263\n",
            "Epoch 21/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1.0370 - val_loss: 1.4236\n",
            "Epoch 22/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1.0573 - val_loss: 1.4175\n",
            "Epoch 23/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1.0142 - val_loss: 1.4245\n",
            "Epoch 24/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.9983 - val_loss: 1.4124\n",
            "Epoch 25/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.9705 - val_loss: 1.4151\n",
            "Epoch 26/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.9870 - val_loss: 1.4038\n",
            "Epoch 27/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.9779 - val_loss: 1.4008\n",
            "Epoch 28/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.9339 - val_loss: 1.3998\n",
            "Epoch 29/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.9309 - val_loss: 1.3954\n",
            "Epoch 30/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.9121 - val_loss: 1.3885\n",
            "Epoch 31/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.9279 - val_loss: 1.3955\n",
            "Epoch 32/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.8985 - val_loss: 1.3936\n",
            "Epoch 33/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.9068 - val_loss: 1.3864\n",
            "Epoch 34/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.8863 - val_loss: 1.3780\n",
            "Epoch 35/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.8718 - val_loss: 1.3783\n",
            "Epoch 36/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.8662 - val_loss: 1.3749\n",
            "Epoch 37/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.8797 - val_loss: 1.3735\n",
            "Epoch 38/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.8255 - val_loss: 1.3797\n",
            "Epoch 39/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.8388 - val_loss: 1.3684\n",
            "Epoch 40/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.8128 - val_loss: 1.3821\n",
            "Epoch 41/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.8323 - val_loss: 1.3753\n",
            "Epoch 42/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.8213 - val_loss: 1.3751\n",
            "Epoch 43/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.8027 - val_loss: 1.3723\n",
            "Epoch 44/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.7683 - val_loss: 1.3755\n",
            "Epoch 45/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.7959 - val_loss: 1.3742\n",
            "Epoch 46/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.7710 - val_loss: 1.3733\n",
            "Epoch 47/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.7565 - val_loss: 1.3771\n",
            "Epoch 48/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.7296 - val_loss: 1.3766\n",
            "Epoch 49/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.7533 - val_loss: 1.3741\n",
            "Epoch 50/50\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.7231 - val_loss: 1.3822\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78366a3649d0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 - Inferencia\n",
        "# Modelo de codificación para inferencia\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Modelo de decodificación para inferencia\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# We need to reuse the decoder's embedding layer here\n",
        "decoder_embedding = Embedding(num_words_output, embedding_dim, weights=[embedding_matrix], trainable=False)\n",
        "decoder_inputs_inference = Input(shape=(None,))\n",
        "decoder_embedding_inference = decoder_embedding(decoder_inputs_inference)\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding_inference, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs_inference] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "# Función para la inferencia\n",
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # en lugar de una string vacía '', uso '<start>' or '<sos>'\n",
        "    # si está presente en el vocabulario\n",
        "    # debo asegurar que ese token se usa durante el entreamiento, para dar consistencia\n",
        "\n",
        "    # por ejemplo, si '<start>' es el token inicial:\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    if '<start>' in word2idx_outputs:\n",
        "        target_seq[0, 0] = word2idx_outputs['<start>']\n",
        "    else:\n",
        "        # retrocedo al índice de la primera palabra si no se encuentra '<inicio>'.\n",
        "        # Suponiendo que el índice 1 representa la primera palabra (ajústelo si es necesario).\n",
        "        target_seq[0, 0] = 1\n",
        "\n",
        "    stop_condition = False\n",
        ""
      ],
      "metadata": {
        "id": "IPYx9Nbx2eYo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de uso\n",
        "input_sentence = \"this is\"\n",
        "input_seq = tokenizer_inputs.texts_to_sequences([input_sentence])\n",
        "input_seq = pad_sequences(input_seq, maxlen=max_input_len, padding='post')\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input:', input_sentence)\n",
        "print('Output:', decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UXU15mS1Ht8",
        "outputId": "9356717f-fd3a-491d-82de-09b2f670655f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step\n",
            "Input: this is\n",
            "Output: None\n"
          ]
        }
      ]
    }
  ]
}