{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9438732,"sourceType":"datasetVersion","datasetId":5735465}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Desafio 3**\n\nCarlos Villalobos\n\n**Consigna**\n\n- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.\n- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.\n\n**Sugerencias**\n\n- Durante el entrenamiento, guiarse por el descenso de la perplejidad en los datos de validación para finalizar el entrenamiento. Para ello se provee un callback.\n- Explorar utilizar SimpleRNN (celda de Elman), LSTM y GRU.\n- rmsprop es el optimizador recomendado para la buena convergencia. No obstante se pueden explorar otros.","metadata":{}},{"cell_type":"markdown","source":"Second try form Claude.","metadata":{}},{"cell_type":"code","source":"import random\nimport io\nimport pickle\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Dropout, SimpleRNN, GRU\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras.utils import pad_sequences\nfrom scipy.special import softmax\n\n# ... [Keep the existing functions: load_corpus, preprocess_corpus, create_model, PplCallback] ...\n\n# 1. Select and load the corpus\ndef load_corpus(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        text = file.read()\n    return text\n\ncorpus = load_corpus('/kaggle/input/songs-dataset/songs_dataset/nirvana.txt')\n\n# 2. Preprocess the corpus\ndef preprocess_corpus(corpus):\n    # Tokenize the text\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts([corpus])\n\n    # Convert text to sequences\n    sequences = tokenizer.texts_to_sequences([corpus])[0]\n\n    # Create input-output pairs\n    input_sequences = []\n    for i in range(1, len(sequences)):\n        n_gram_sequence = sequences[:i+1]\n        input_sequences.append(n_gram_sequence)\n\n    # Pad sequences\n    max_sequence_length = max([len(seq) for seq in input_sequences])\n    input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n\n    # Create predictors and label\n    predictors, label = input_sequences[:,:-1], input_sequences[:,-1]\n    label = to_categorical(label, num_classes=len(tokenizer.word_index) + 1)\n\n    return predictors, label, tokenizer, max_sequence_length\n\npredictors, label, tokenizer, max_sequence_length = preprocess_corpus(corpus)\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(predictors, label, test_size=0.2, random_state=42)\n\n# 3. Define model architectures\ndef create_model(model_type, vocab_size, max_sequence_length):\n    model = Sequential()\n    model.add(Embedding(vocab_size, 50, input_length=max_sequence_length-1))\n\n    if model_type == 'SimpleRNN':\n        model.add(SimpleRNN(100, return_sequences=True))\n        model.add(SimpleRNN(100))\n    elif model_type == 'LSTM':\n        model.add(LSTM(100, return_sequences=True))\n        model.add(LSTM(100))\n    elif model_type == 'GRU':\n        model.add(GRU(100, return_sequences=True))\n        model.add(GRU(100))\n\n    model.add(Dense(vocab_size, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n    return model\n\n# Perplexity Callback\nclass PplCallback(keras.callbacks.Callback):\n    def __init__(self, val_data, patience=5):\n        self.val_data = val_data\n        self.patience = patience\n        self.min_perplexity = float('inf')\n        self.wait = 0\n        self.best_weights = None\n\n    def on_epoch_end(self, epoch, logs=None):\n        val_loss = logs.get('val_loss')\n        perplexity = np.exp(val_loss)\n        print(f'\\nValidation Perplexity: {perplexity:.4f}')\n\n        if perplexity < self.min_perplexity:\n            self.min_perplexity = perplexity\n            self.wait = 0\n            self.best_weights = self.model.get_weights()\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                self.model.stop_training = True\n                print(\"Restoring model weights from the end of the best epoch.\")\n                self.model.set_weights(self.best_weights)\n\n# Modify the training loop\nmodels = {}\nhistories = {}\nfor model_type in ['SimpleRNN', 'LSTM', 'GRU']:\n    print(f\"\\nTraining {model_type} model:\")\n    model = create_model(model_type, len(tokenizer.word_index) + 1, max_sequence_length)\n    ppl_callback = PplCallback(val_data=(X_val, y_val))\n\n    batch_size = 64\n    epochs = 2  # Reduce epochs if still encountering memory issues\n\n    history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val),\n                        callbacks=[ppl_callback], batch_size=batch_size, verbose=1)\n    models[model_type] = model\n    histories[model_type] = history\n\n# Function to generate sequence with timeout\ndef generate_sequence_with_timeout(model, tokenizer, seed_text, max_length, num_words, temperature=1.0, mode='greedy', timeout=10):\n    start_time = time.time()\n    while time.time() - start_time < timeout:\n        try:\n            return generate_sequence(model, tokenizer, seed_text, max_length, num_words, temperature, mode)\n        except Exception as e:\n            print(f\"Error in generation: {e}\")\n            return f\"Error: {str(e)}\"\n    return \"Timeout: Generation took too long\"\n\n# Generate sequences using different methods\nseed_text = \"Come as you are\"\nfor model_type, model in models.items():\n    print(f\"\\n{model_type} Model Generation:\")\n    result = generate_sequence_with_timeout(model, tokenizer, seed_text, max_sequence_length, 10, 1.0, 'greedy')\n    print(result)\n\n# Plot training history\nplt.figure(figsize=(12, 8))\nfor model_type, history in histories.items():\n    plt.plot(history.history['val_loss'], label=f'{model_type} Validation Loss')\nplt.title('Model Validation Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend()\nplt.savefig('validation_loss.png')\nplt.close()\n\nprint(\"Validation loss graph saved as 'validation_loss.png'\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Otra vez","metadata":{}},{"cell_type":"code","source":"import random\nimport io\nimport pickle\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Dropout, SimpleRNN, GRU\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras.utils import pad_sequences\nfrom scipy.special import softmax\nimport time\n\n# 1. Select and load the corpus\ndef load_corpus(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        text = file.read()\n    return text\n\ncorpus = load_corpus('/kaggle/input/songs-dataset/songs_dataset/nirvana.txt')\n\n# 2. Preprocess the corpus\ndef preprocess_corpus(corpus):\n    # Tokenize the text\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts([corpus])\n\n    # Convert text to sequences\n    sequences = tokenizer.texts_to_sequences([corpus])[0]\n\n    # Create input-output pairs\n    input_sequences = []\n    for i in range(1, len(sequences)):\n        n_gram_sequence = sequences[:i+1]\n        input_sequences.append(n_gram_sequence)\n\n    # Pad sequences\n    max_sequence_length = max([len(seq) for seq in input_sequences])\n    input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n\n    # Create predictors and label\n    predictors, label = input_sequences[:,:-1], input_sequences[:,-1]\n    label = to_categorical(label, num_classes=len(tokenizer.word_index) + 1)\n\n    return predictors, label, tokenizer, max_sequence_length\n\npredictors, label, tokenizer, max_sequence_length = preprocess_corpus(corpus)\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(predictors, label, test_size=0.2, random_state=42)\n\n# 3. Define model architectures\ndef create_model(model_type, vocab_size, max_sequence_length):\n    model = Sequential()\n    model.add(Embedding(vocab_size, 50, input_length=max_sequence_length-1))\n\n    if model_type == 'SimpleRNN':\n        model.add(SimpleRNN(100, return_sequences=True))\n        model.add(SimpleRNN(100))\n    elif model_type == 'LSTM':\n        model.add(LSTM(100, return_sequences=True))\n        model.add(LSTM(100))\n    elif model_type == 'GRU':\n        model.add(GRU(100, return_sequences=True))\n        model.add(GRU(100))\n\n    model.add(Dense(vocab_size, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n    return model\n\n# Perplexity Callback\nclass PplCallback(keras.callbacks.Callback):\n    def __init__(self, val_data, patience=5):\n        self.val_data = val_data\n        self.patience = patience\n        self.min_perplexity = float('inf')\n        self.wait = 0\n        self.best_weights = None\n\n    def on_epoch_end(self, epoch, logs=None):\n        val_loss = logs.get('val_loss')\n        perplexity = np.exp(val_loss)\n        print(f'\\nValidation Perplexity: {perplexity:.4f}')\n\n        if perplexity < self.min_perplexity:\n            self.min_perplexity = perplexity\n            self.wait = 0\n            self.best_weights = self.model.get_weights()\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                self.model.stop_training = True\n                print(\"Restoring model weights from the end of the best epoch.\")\n                self.model.set_weights(self.best_weights)\n\n# Generate sequence function\ndef generate_sequence(model, tokenizer, seed_text, max_length, num_words, temperature=1.0, mode='greedy'):\n    for _ in range(num_words):\n        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n        encoded = pad_sequences([encoded], maxlen=max_length-1, padding='pre')\n\n        if mode == 'greedy':\n            predicted = model.predict(encoded, verbose=0)\n            predicted = np.argmax(predicted, axis=-1)\n        elif mode == 'stochastic':\n            predicted = model.predict(encoded, verbose=0)\n            predicted = sample_with_temperature(predicted[0], temperature)\n\n        output_word = \"\"\n        for word, index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        seed_text += \" \" + output_word\n    return seed_text\n\ndef sample_with_temperature(preds, temperature):\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)\n\n# Train models\nmodels = {}\nhistories = {}\nfor model_type in ['SimpleRNN', 'LSTM', 'GRU']:\n    print(f\"\\nTraining {model_type} model:\")\n    model = create_model(model_type, len(tokenizer.word_index) + 1, max_sequence_length)\n    ppl_callback = PplCallback(val_data=(X_val, y_val))\n\n    batch_size = 64\n    epochs = 2  # Reduced epochs for faster execution\n\n    history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val),\n                        callbacks=[ppl_callback], batch_size=batch_size, verbose=1)\n    models[model_type] = model\n    histories[model_type] = history\n\n# Function to generate sequence with timeout\ndef generate_sequence_with_timeout(model, tokenizer, seed_text, max_length, num_words, temperature=1.0, mode='greedy', timeout=10):\n    start_time = time.time()\n    while time.time() - start_time < timeout:\n        try:\n            return generate_sequence(model, tokenizer, seed_text, max_length, num_words, temperature, mode)\n        except Exception as e:\n            print(f\"Error in generation: {e}\")\n            return f\"Error: {str(e)}\"\n    return \"Timeout: Generation took too long\"\n\n# Generate sequences using different methods\nseed_text = \"Come as you are\"\nfor model_type, model in models.items():\n    print(f\"\\n{model_type} Model Generation:\")\n    result = generate_sequence_with_timeout(model, tokenizer, seed_text, max_sequence_length, 10, 1.0, 'greedy')\n    print(result)\n\n# Plot training history\nplt.figure(figsize=(12, 8))\nfor model_type, history in histories.items():\n    plt.plot(history.history['val_loss'], label=f'{model_type} Validation Loss')\nplt.title('Model Validation Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend()\nplt.savefig('validation_loss.png')\nplt.close()\n\nprint(\"Validation loss graph saved as 'validation_loss.png'\")","metadata":{"execution":{"iopub.status.busy":"2024-09-26T22:38:13.651174Z","iopub.execute_input":"2024-09-26T22:38:13.651541Z","iopub.status.idle":"2024-09-26T23:04:51.376645Z","shell.execute_reply.started":"2024-09-26T22:38:13.651505Z","shell.execute_reply":"2024-09-26T23:04:51.375693Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\nTraining SimpleRNN model:\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/2\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1727390307.937278    1118 service.cc:145] XLA service 0x7fb588012d60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1727390307.937344    1118 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1727390307.937351    1118 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1727390309.999538    1118 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.0197 - loss: 6.7911\nValidation Perplexity: 419.2848\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 4s/step - accuracy: 0.0197 - loss: 6.7882 - val_accuracy: 0.0381 - val_loss: 6.0386\nEpoch 2/2\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.0409 - loss: 5.9266\nValidation Perplexity: 413.0592\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m373s\u001b[0m 2s/step - accuracy: 0.0409 - loss: 5.9267 - val_accuracy: 0.0296 - val_loss: 6.0236\n\nTraining LSTM model:\nEpoch 1/2\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867ms/step - accuracy: 0.0319 - loss: 6.5342\nValidation Perplexity: 422.7978\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 964ms/step - accuracy: 0.0319 - loss: 6.5319 - val_accuracy: 0.0381 - val_loss: 6.0469\nEpoch 2/2\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867ms/step - accuracy: 0.0398 - loss: 5.9240\nValidation Perplexity: 407.0432\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 962ms/step - accuracy: 0.0398 - loss: 5.9241 - val_accuracy: 0.0381 - val_loss: 6.0089\n\nTraining GRU model:\nEpoch 1/2\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840ms/step - accuracy: 0.0318 - loss: 6.5552\nValidation Perplexity: 411.8521\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 927ms/step - accuracy: 0.0319 - loss: 6.5529 - val_accuracy: 0.0381 - val_loss: 6.0207\nEpoch 2/2\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840ms/step - accuracy: 0.0378 - loss: 5.8897\nValidation Perplexity: 380.7685\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 924ms/step - accuracy: 0.0378 - loss: 5.8898 - val_accuracy: 0.0381 - val_loss: 5.9422\n\nSimpleRNN Model Generation:\nCome as you are to to to to to to to to to to\n\nLSTM Model Generation:\nCome as you are i i i i i i i i i i\n\nGRU Model Generation:\nCome as you are i i i i i i i i i i\nValidation loss graph saved as 'validation_loss.png'\n","output_type":"stream"}]}]}